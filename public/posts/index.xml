<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>Posts on Yuankun&#39;s Blog</title>
		<link>https://yuankun.me/posts/</link>
		<description>Recent content in Posts on Yuankun&#39;s Blog</description>
		<generator>Hugo -- gohugo.io</generator>
		<language>en-us</language>
		<copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright>
		<lastBuildDate>Wed, 01 Apr 2020 23:47:03 +0800</lastBuildDate>
		<atom:link href="https://yuankun.me/posts/index.xml" rel="self" type="application/rss+xml" />
		
		<item>
			<title>A Terraform Module to List Google Cloud Service Agents</title>
			<link>https://yuankun.me/posts/a-terraform-module-to-list-google-cloud-service-agents/</link>
			<pubDate>Wed, 01 Apr 2020 23:47:03 +0800</pubDate>
			
			<guid>https://yuankun.me/posts/a-terraform-module-to-list-google-cloud-service-agents/</guid>
			<description>There are two types of service accounts in Google Cloud: user-managed service accounts, which are used by user applications to talk to Google Cloud; and Google-managed services accounts, which are used by Google Cloud internally. Among the second category, there is a special subtype of service accounts called Google Cloud Service Agents. Service Agents are used by Google Cloud services to run internal processes so that user requested operations can be fulfilled.</description>
			<content type="html"><![CDATA[<p>There are <a href="https://cloud.google.com/iam/docs/service-accounts#types_of_service_accounts">two types of service accounts</a> in Google Cloud: user-managed service accounts, which are used by user applications to talk to Google Cloud; and Google-managed services accounts, which are used by Google Cloud internally. Among the second category, there is a special subtype of service accounts called Google Cloud Service Agents. Service Agents are used by Google Cloud services to run internal processes so that user requested operations can be fulfilled.</p>
<p>A service agent has the following pattern:</p>
<pre><code>service-PROJECT_NUMBER@SERVICE_NAME.iam.gserviceaccount.com
</code></pre><p>You can spot the service agents from the IAM section of Google Cloud Console.</p>
<p><img src="/img/20200402-0012.png" alt="Service Agents"></p>
<p>When managing IAM binding policies via Terraform, these service agents often generate noises. As an example, I&rsquo;ll show you a code snippet coming from one of our Terraform files (I&rsquo;m using <code>xxxxx</code> instead of the real project number).</p>
<div class="highlight"><pre class="chroma"><code class="language-hcl" data-lang="hcl"><span class="k">data</span> <span class="s2">&#34;google_project&#34; &#34;project&#34;</span> {}

<span class="k">resource</span> <span class="s2">&#34;google_project_iam_binding&#34; &#34;cloudbuild_service_agent&#34;</span> {
<span class="n">  project</span> <span class="o">=</span> <span class="k">data</span><span class="p">.</span><span class="k">google_project</span><span class="p">.</span><span class="k">project</span><span class="p">.</span><span class="k">project_id</span>
<span class="n">  role</span>    <span class="o">=</span> <span class="s2">&#34;roles/cloudbuild.serviceAgent&#34;</span>

<span class="n">  members</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&#34;serviceAccount:service-xxxxx@gcp-sa-cloudbuild.iam.gserviceaccount.com&#34;</span><span class="p">,</span>
  <span class="p">]</span>
}

<span class="k">resource</span> <span class="s2">&#34;google_project_iam_binding&#34; &#34;container_service_agent&#34;</span> {
<span class="n">  project</span> <span class="o">=</span> <span class="k">data</span><span class="p">.</span><span class="k">google_project</span><span class="p">.</span><span class="k">project</span><span class="p">.</span><span class="k">project_id</span>
<span class="n">  role</span>    <span class="o">=</span> <span class="s2">&#34;roles/container.serviceAgent&#34;</span>

<span class="n">  members</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&#34;serviceAccount:service-xxxxx@container-engine-robot.iam.gserviceaccount.com&#34;</span><span class="p">,</span>
  <span class="p">]</span>
}
</code></pre></div><p>When you are refering to service agents from other projects, things get even worse. Those project numbers look very much lick what we call &ldquo;the magic number&rdquo;.</p>
<p>To tackle this, I wrote a simple Terraform module named <a href="https://github.com/yuankunzhang/google-cloud-service-agents">google-cloud-service-agents</a>. It consumes a project ID and exposes a list of service agents. With this module, the above code snippet can be rewritten to this:</p>
<div class="highlight"><pre class="chroma"><code class="language-hcl" data-lang="hcl"><span class="k">data</span> <span class="s2">&#34;google_project&#34; &#34;project&#34;</span> {}

<span class="k">module</span> <span class="s2">&#34;agents&#34;</span> {
<span class="n">  source</span> <span class="o">=</span> <span class="s2">&#34;/path/to/module&#34;</span>

<span class="n">  project_number</span> <span class="o">=</span> <span class="k">data</span><span class="p">.</span><span class="k">google_project</span><span class="p">.</span><span class="k">project</span><span class="p">.</span><span class="k">number</span>
}

<span class="k">resource</span> <span class="s2">&#34;google_project_iam_binding&#34; &#34;cloudbuild_service_agent&#34;</span> {
<span class="n">  project</span> <span class="o">=</span> <span class="k">data</span><span class="p">.</span><span class="k">google_project</span><span class="p">.</span><span class="k">project</span><span class="p">.</span><span class="k">project_id</span>
<span class="n">  role</span>    <span class="o">=</span> <span class="s2">&#34;roles/cloudbuild.serviceAgent&#34;</span>

<span class="n">  members</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&#34;serviceAccount:${module.agents.cloud_build}&#34;</span><span class="p">,</span>
  <span class="p">]</span>
}

<span class="k">resource</span> <span class="s2">&#34;google_project_iam_binding&#34; &#34;container_service_agent&#34;</span> {
<span class="n">  project</span> <span class="o">=</span> <span class="k">data</span><span class="p">.</span><span class="k">google_project</span><span class="p">.</span><span class="k">project</span><span class="p">.</span><span class="k">project_id</span>
<span class="n">  role</span>    <span class="o">=</span> <span class="s2">&#34;roles/container.serviceAgent&#34;</span>

<span class="n">  members</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&#34;serviceAccount:${module.agents.container_engine}&#34;</span><span class="p">,</span>
  <span class="p">]</span>
}
</code></pre></div><p>For more information, please check out the <a href="https://github.com/yuankunzhang/google-cloud-service-agents">Github repository</a>.</p>
]]></content>
		</item>
		
		<item>
			<title>GCP IPs All Appear in US, and It Is an Intended Behavior</title>
			<link>https://yuankun.me/posts/gcp-ips-all-appear-in-us-and-it-is-an-intended-behavior/</link>
			<pubDate>Sun, 22 Mar 2020 23:12:59 +0800</pubDate>
			
			<guid>https://yuankun.me/posts/gcp-ips-all-appear-in-us-and-it-is-an-intended-behavior/</guid>
			<description>As you may have already known, all the IPs of Google Cloud Platform share the same geolocation, which is the US. This buggy thing has been marked as &amp;ldquo;intended behavior&amp;rdquo;, so no fix would be expected in the future. Even ironically, the guy who raised this issue was &amp;ldquo;kindly advised&amp;rdquo; to contact Google&amp;rsquo;s legal department. Here is the issue ticket.
Dear Google, when people create a network in a chosen region, they expect the public IPs of the network to be in that region.</description>
			<content type="html"><![CDATA[<p>As you may have already known, all the IPs of Google Cloud Platform share the same geolocation, which is the US. This buggy thing has been marked as &ldquo;intended behavior&rdquo;, so no fix would be expected in the future. Even ironically, the guy who raised this issue was &ldquo;kindly advised&rdquo; to contact Google&rsquo;s legal department. Here is the <a href="https://issuetracker.google.com/issues/112448138">issue ticket</a>.</p>
<p><img src="/img/you-are-kindly-advised-to-contact-googles-legal-department.png" alt="You are kindly advised to contact Google&rsquo;s legal department"></p>
<p>Dear Google, when people create a network in a chosen region, they expect the public IPs of the network to be in that region. Let&rsquo;s take the GDPR thing as an example. You set up a service in Europe region, but a geolocation lookup of the service IP shows it is in the US. Even though the service is indeed running in Europe region, you are gonna need some effort to convince your customers that there should be no trace that the data is going out of Europe.</p>
<p>I can tell another issue caused by this &ldquo;feature&rdquo;. A few days ago, a friend of mine encountered a weird challenge. They have servers running on Google Cloud Platform in both Asia and North America, serving Asian and North American customers respectively. Recently they integrated a new third party service (let&rsquo;s call it TPS) to their backend services. This TPS is claimed to have replicas running in both Asia and North America. So requests originating from Asia should be hitting the TPS replica in Asia, and requests originating from North America should be hitting the TPS replica in North America. Out of expectation, all requests were going to the TPS replica in North America. This adds about 200ms extra latency to every customer request!</p>
<p><img src="/img/unexpected-traffic-routing.png" alt="Unexpected traffic routing"></p>
<p>And there are other people facing the same issue.</p>
<p><a href="https://stackoverflow.com/questions/58108523/outgoing-http-request-location-on-google-app-engine">This Stackoverflow user</a> migrated his service from DigitalOcean to GCP and the service didn&rsquo;t work healthily, because &ldquo;a third-party API only accepts requests coming from Brazil&rdquo;.</p>
<p><a href="https://stackoverflow.com/questions/51801691/created-gce-in-europe-region-but-ip-address-shows-its-in-us">This Stackoverflow user</a> gave up using GCP and turned to Azure, because &ldquo;they have strict guideline not to transmit data out of Europe region and they couldn&rsquo;t prove to customers that their service is actually in Europe not in US&rdquo;.</p>
<p>Nowadays lots of existing infrastructure (firewalls, CDNs, etc) and services rely on IP geolocation. The &ldquo;all IPs sharing same geolocation feature&rdquo; for sure fools them all.</p>
<p>And I&rsquo;m waiting for more victims.</p>
]]></content>
		</item>
		
		<item>
			<title>Debug Linux Kernel With QEMU and GDB</title>
			<link>https://yuankun.me/posts/debug-linux-kernel-with-qemu-and-gdb/</link>
			<pubDate>Fri, 20 Mar 2020 09:31:33 +0800</pubDate>
			
			<guid>https://yuankun.me/posts/debug-linux-kernel-with-qemu-and-gdb/</guid>
			<description>&lt;p&gt;In last post we see how to run a raw Linux kernel in QEMU. QEMU offers another fancy feature: it can start a GDB Server and external GDB Debugger to connect. With this we can build a comfortable environment to debug system kernels and firmware. Let&amp;rsquo;s see how to leverage this feature to debug the Linux kernel.&lt;/p&gt;</description>
			<content type="html"><![CDATA[<p>In last post we see how to run a raw Linux kernel in QEMU. QEMU offers another fancy feature: it can start a GDB Server and external GDB Debugger to connect. With this we can build a comfortable environment to debug system kernels and firmware. Let&rsquo;s see how to leverage this feature to debug the Linux kernel.</p>
<h2 id="compiling-the-kernel-with-debug-info">Compiling the Kernel with Debug Info</h2>
<p>First thing we need to do is to prepare a kernel with debug info. Enter the TUI kernel configuration interface:</p>
<div class="highlight"><pre class="chroma"><code class="language-sh" data-lang="sh">$ <span class="nb">cd</span> linux-source/
$ make menuconfig
</code></pre></div><p>Enter &ldquo;Kernel hacking &gt; Compile-time checks and compiler options&rdquo;, and enable these two options:</p>
<ul>
<li>Compile the kernel with debug info</li>
<li>Provide GDB scripts for kernel debugging</li>
</ul>
<p><img src="/img/provide-gdb-scripts-for-kernel-debugging.png" alt="Provide GDB scripts for kernel debugging"></p>
<p>Save the new configuration and compile the kernel by invoking <code>make -j8</code>. After the compilation, we are interested in two newly created files:</p>
<ul>
<li>vmlinux. This is the Linux Kernel in an statically linked executable file format with all debugging information.</li>
<li>scripts/gdb/vmlinux-gdb.py. This is the GDB script for kernel debugging.</li>
</ul>
<p>Let&rsquo;s add the GDB script to the GDB init file so that the script gets loaded everytime we start GDB Debugger:</p>
<div class="highlight"><pre class="chroma"><code class="language-sh" data-lang="sh">$ <span class="nb">echo</span> <span class="s2">&#34;add-auto-load-safe-path `pwd`/scripts/gdb/vmlinux-gdb.py&#34;</span> &gt;&gt; ~/.gdbinit
</code></pre></div><h2 id="start-a-debugging-session">Start a Debugging Session</h2>
<p>QEMU provides two important options for debugging purpose:</p>
<ul>
<li>The <code>-S</code> option prevents the CPU from starting. This gives time for debugger to connect and allows to start debugging from the very beginning.</li>
<li>The <code>-s</code> option starts a GDB Server on port 1234. Later in GDB Debugger we can connect to it with <code>target remote :1234</code>.</li>
</ul>
<p>Now let&rsquo;s boot the kernel with these options:</p>
<div class="highlight"><pre class="chroma"><code class="language-sh" data-lang="sh">$ qemu-system-x86_64 <span class="se">\
</span><span class="se"></span>    -S <span class="se">\
</span><span class="se"></span>    -s <span class="se">\
</span><span class="se"></span>    -enable-kvm <span class="se">\
</span><span class="se"></span>    -kernel bzImage <span class="se">\
</span><span class="se"></span>    -smp <span class="nv">cores</span><span class="o">=</span>1,threads<span class="o">=</span><span class="m">2</span> <span class="se">\
</span><span class="se"></span>    -m <span class="m">1024</span> <span class="se">\
</span><span class="se"></span>    -append <span class="s2">&#34;console=ttyS0 nokaslr selinux=0 debug&#34;</span> <span class="se">\
</span><span class="se"></span>    -initrd initramfs.img <span class="se">\
</span><span class="se"></span>    -serial stdio <span class="se">\
</span><span class="se"></span>    -display none
</code></pre></div><p>Note that the running of the process is feezed, because we have told QEMU to wait for debugger by using the <code>-S</code> option.</p>
<p>In another terminal, start the GDB Debugger and connect it to QEMU:</p>
<div class="highlight"><pre class="chroma"><code class="language-sh" data-lang="sh">$ gdb vmlinux
Type <span class="s2">&#34;apropos word&#34;</span> to search <span class="k">for</span> commands related to <span class="s2">&#34;word&#34;</span>...
Reading symbols from vmlinux...
<span class="o">(</span>gdb<span class="o">)</span> target remote :1234
Remote debugging using :1234
0x000000000000fff0 in exception_stacks <span class="o">()</span>
</code></pre></div><p>Now we are able to set break points and trace the running of the kernel as if it is just a normal user application:</p>
<div class="highlight"><pre class="chroma"><code class="language-sh" data-lang="sh"><span class="o">(</span>gdb<span class="o">)</span> b start_kernel
Note: breakpoints <span class="m">1</span> and <span class="m">2</span> also <span class="nb">set</span> at pc 0xffffffff829e0aa8.
Breakpoint <span class="m">3</span> at 0xffffffff829e0aa8: file init/main.c, line 786.
<span class="o">(</span>gdb<span class="o">)</span> c
Continuing.

Thread <span class="m">1</span> hit Breakpoint 1, start_kernel <span class="o">()</span> at init/main.c:786
<span class="m">786</span>     <span class="o">{</span>
...
</code></pre></div><h2 id="references">References</h2>
<ul>
<li><a href="https://en.wikibooks.org/wiki/QEMU/Debugging_with_QEMU">Debugging with QEMU</a></li>
</ul>]]></content>
		</item>
		
		<item>
			<title>Running Raw Linux Kernel in QEMU</title>
			<link>https://yuankun.me/posts/running-raw-linux-kernel-in-qemu/</link>
			<pubDate>Mon, 16 Mar 2020 22:50:52 +0800</pubDate>
			
			<guid>https://yuankun.me/posts/running-raw-linux-kernel-in-qemu/</guid>
			<description>&lt;p&gt;In last post we see how to run a packed Linux distribution in QEMU. This time let&amp;rsquo;s check out how to run a raw Linux kernel in QEMU.&lt;/p&gt;</description>
			<content type="html"><![CDATA[<p>In last post we see how to run a packed Linux distribution in QEMU. This time let&rsquo;s check out how to run a raw Linux kernel in QEMU.</p>
<h2 id="initial-ramdisk-a-very-short-introduction">Initial ramdisk: a very short introduction</h2>
<p>Many Linux distributions ship a small, generic kernel image. The device drivers are included as loadable kernel modules and stored in file system, it is just not practical to bake all the device drivers into the kernel image. On my machine, the size of <code>vmlinuz-linux</code> is only 6.3 megabytes:</p>
<div class="highlight"><pre class="chroma"><code class="language-sh" data-lang="sh">$ ls -lh /boot/vmlinuz-linux
-rw-r--r-- <span class="m">1</span> root root 6.3M Mar <span class="m">15</span> 21:30 /boot/vmlinuz-linux
</code></pre></div><p>This raises the problem of detecting and loading the modules necessary to mount the root file system at boot time. It&rsquo;s a chicken-and-egg problem. To further complicate the situation, the root file system may require special preparations to mount (for instance, it is on a encrypted partition).</p>
<p>Now comes the initial ramdisk as a temporary, ram-based root file system. It contains user-space utilities which detect hardwares, descover devices, load necessary modules, and mount the real root file system. Once it is loaded into memory, a simple but sufficient environment is set up for the Linux kernel to complete the boot process. This environment is often called &ldquo;early user space&rdquo;.</p>
<p>On my machine, the initial ramdisk image sits in the boot partition with the name <code>initramfs-linux.img</code>. It is larger than the Linux kernel image:</p>
<div class="highlight"><pre class="chroma"><code class="language-sh" data-lang="sh">$ ls -lh /boot/initramfs-linux.img
-rw-r--r-- <span class="m">1</span> root root 9.4M Mar <span class="m">15</span> 21:30 /boot/initramfs-linux.img
</code></pre></div><p>We can inspect contents of the image by using command <code>lsinitcpio /boot/initramfs-linux.img</code>. Indeed, it is a simplified root system with a bunch of helper tools:</p>
<div class="highlight"><pre class="chroma"><code class="language-sh" data-lang="sh">$ lsinitcpio /boot/initramfs-linux.img
bin
buildconfig
config
dev/
etc/
etc/fstab
etc/initrd-release
etc/ld.so.cache
etc/ld.so.conf
etc/modprobe.d/
etc/mtab
hooks/
hooks/udev
init
init_functions
lib
lib64
new_root/
proc/
run/
sbin
sys/
tmp/
usr/
usr/bin/
...
var/
var/run
VERSION
</code></pre></div><h2 id="making-an-initial-ramdisk">Making an initial ramdisk</h2>
<p>The initial ramdisk creation command in Arch Linux is <code>mkinitcpio</code>, and may defer in other distributions.</p>
<p>Quoting from man page of <code>mkinitcpio</code>:</p>
<blockquote>
<p>(mkinitcpio) creates an initial ramdisk environment for booting the Linux kernel. The initial ramdisk is in essence a very small environment (early userspace) which loads various kernel modules and sets up necessary things before handing over control to init. This makes it possible to have, for example, encrypted root filesystems and root filesystems on a software RAID array. mkinitcpio allows for easy extension with custom hooks, has autodetection at runtime, and many other features.</p>
</blockquote>
<p>Let&rsquo;s go ahead and create the initial ramdisk.</p>
<div class="highlight"><pre class="chroma"><code class="language-sh" data-lang="sh">$ mkinitcpio -g initramfs.img
<span class="o">==</span>&gt; Starting build: 5.5.9-arch1-2
  -&gt; Running build hook: <span class="o">[</span>base<span class="o">]</span>
  -&gt; Running build hook: <span class="o">[</span>udev<span class="o">]</span>
  -&gt; Running build hook: <span class="o">[</span>autodetect<span class="o">]</span>
  -&gt; Running build hook: <span class="o">[</span>modconf<span class="o">]</span>
  -&gt; Running build hook: <span class="o">[</span>block<span class="o">]</span>
  -&gt; Running build hook: <span class="o">[</span>filesystems<span class="o">]</span>
  -&gt; Running build hook: <span class="o">[</span>keyboard<span class="o">]</span>
  -&gt; Running build hook: <span class="o">[</span>fsck<span class="o">]</span>
<span class="o">==</span>&gt; Generating module <span class="nv">dependencies</span>
<span class="o">==</span>&gt; Creating gzip-compressed initcpio image: /home/yuankun/qemu-test/initramfs.img
<span class="o">==</span>&gt; Image generation successful
</code></pre></div><h2 id="configuring-and-building-the-linux-kernel">Configuring and building the Linux kernel</h2>
<p>Clone the Linux source code, and configure the Linux kernel with <code>make ARCH=x86_64 menuconfig</code>.</p>
<p><img src="/img/linux-menuconfig.png" alt="Menuconfig"></p>
<p>Save the configurations and exit the configuration interface, now let&rsquo;s compile the kernel image. The compiled kernel image will be located at <code>arch/x86/boot/bzImage</code>.</p>
<div class="highlight"><pre class="chroma"><code class="language-sh" data-lang="sh">$ make -j8
...
Setup is <span class="m">13820</span> bytes <span class="o">(</span>padded to <span class="m">13824</span> bytes<span class="o">)</span>.
System is <span class="m">8801</span> kB
CRC 52c54fbc
Kernel: arch/x86/boot/bzImage is ready  <span class="o">(</span><span class="c1">#2)</span>
</code></pre></div><p>The <code>-j</code> option specifies the number of jobs (commands) to run simultaneously. It&rsquo;s a reasonable choice to match it with your available logical CPU cores.</p>
<h2 id="booting-the-linux-kernel-in-qemu">Booting the Linux kernel in QEMU</h2>
<p>Now that we have both the Kernel image and the initial ramdisk, it&rsquo;s time to boot the Linux kernel in QEMU.</p>
<div class="highlight"><pre class="chroma"><code class="language-sh" data-lang="sh">$ qemu-system-x86_64 <span class="se">\
</span><span class="se"></span>    -enable-kvm <span class="se">\
</span><span class="se"></span>    -kernel bzImage <span class="se">\
</span><span class="se"></span>    -smp <span class="nv">cores</span><span class="o">=</span>1,threads<span class="o">=</span><span class="m">2</span> <span class="se">\
</span><span class="se"></span>    -m <span class="m">1024</span> <span class="se">\
</span><span class="se"></span>    -append <span class="s2">&#34;console=ttyS0&#34;</span> <span class="se">\
</span><span class="se"></span>    -initrd initramfs.img <span class="se">\
</span><span class="se"></span>    -serial stdio <span class="se">\
</span><span class="se"></span>    -display none
<span class="o">[</span>    0.000000<span class="o">]</span> Linux version 5.6.0-rc6+ <span class="o">(</span>yuankun@mars<span class="o">)</span> <span class="o">(</span>gcc version 9.3.0 <span class="o">(</span>Arch Linux 9.3.0-1<span class="o">))</span> <span class="c1">#2 SMP Tue Mar 17 17:42:13 +08 2020</span>
<span class="o">[</span>    0.000000<span class="o">]</span> Command line: <span class="nv">console</span><span class="o">=</span>ttyS0
<span class="o">[</span>    0.000000<span class="o">]</span> x86/fpu: x87 FPU will use FXSAVE
<span class="o">[</span>    0.000000<span class="o">]</span> BIOS-provided physical RAM map:
<span class="o">[</span>    0.000000<span class="o">]</span> BIOS-e820: <span class="o">[</span>mem 0x0000000000000000-0x000000000009fbff<span class="o">]</span> usable
<span class="o">[</span>    0.000000<span class="o">]</span> BIOS-e820: <span class="o">[</span>mem 0x000000000009fc00-0x000000000009ffff<span class="o">]</span> reserved
<span class="o">[</span>    0.000000<span class="o">]</span> BIOS-e820: <span class="o">[</span>mem 0x00000000000f0000-0x00000000000fffff<span class="o">]</span> reserved
<span class="o">[</span>    0.000000<span class="o">]</span> BIOS-e820: <span class="o">[</span>mem 0x0000000000100000-0x000000007ffdffff<span class="o">]</span> usable
<span class="o">[</span>    0.000000<span class="o">]</span> BIOS-e820: <span class="o">[</span>mem 0x000000007ffe0000-0x000000007fffffff<span class="o">]</span> reserved
<span class="o">[</span>    0.000000<span class="o">]</span> BIOS-e820: <span class="o">[</span>mem 0x00000000feffc000-0x00000000feffffff<span class="o">]</span> reserved
<span class="o">[</span>    0.000000<span class="o">]</span> BIOS-e820: <span class="o">[</span>mem 0x00000000fffc0000-0x00000000ffffffff<span class="o">]</span> reserved
<span class="o">[</span>    0.000000<span class="o">]</span> NX <span class="o">(</span>Execute Disable<span class="o">)</span> protection: active
<span class="o">[</span>    0.000000<span class="o">]</span> SMBIOS 2.8 present.
...
<span class="o">[</span>rootfs <span class="o">]</span>#
</code></pre></div><p>Soon you will find the <code>[rootfs] #</code> prompt appearing, and cheers we are now in the environment provided by the initial ramdisk.</p>]]></content>
		</item>
		
		<item>
			<title>Running Alpine Linux in QEMU</title>
			<link>https://yuankun.me/posts/running-alpine-linux-in-qemu/</link>
			<pubDate>Sat, 14 Mar 2020 09:41:41 +0800</pubDate>
			
			<guid>https://yuankun.me/posts/running-alpine-linux-in-qemu/</guid>
			<description>&lt;p&gt;How to run a Linux operating system in QEMU.&lt;/p&gt;</description>
			<content type="html"><![CDATA[<p>How to run a Linux operating system in QEMU.</p>
<h2 id="qemu-a-very-short-introduction">QEMU: a very short introduction</h2>
<p>According to its <a href="https://www.qemu.org">official site</a>, QEMU is a generic and open source machine emulator and virtualizer.</p>
<p>QEMU works in one of the two operating modes:</p>
<ul>
<li><strong>Full system emulation</strong>. In this mode, QEMU emulates a full system, including processors and peripherals. It can be used to launch virtual guest operating systems.</li>
<li><strong>User mode emulation</strong>. In this mode, QEMU can launch processes that were compiled for a different instruction set.</li>
</ul>
<p>Futhermore, you can provision the <code>-enable-kvm</code> option to leverage Linux KVM. With this option, QEMU deals with the setting up and migration of KVM images and emulates hardwares, and the execution of the guest is done by KVM as requested by QEMU.</p>
<p>QEMU supports a long <a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/virtualization_deployment_and_administration_guide/sect-using_qemu_img-supported_qemu_img_formats">list of disk image formats</a>, including the most popular two:</p>
<ul>
<li>Raw images (<code>.img</code>). This can be the fastest file-based format. If your file system supports holes, then only the written sectors will reserve space. Use <code>qemu-img info</code> or <code>ls -ls</code> to obtain the real size used by the image. Although raw images give optimal performance, only very basic features are available.</li>
<li>QEMU copy on write (<code>.qcow2</code>). This is the most versatile format with advanced feature set. But the feature set comes at the cost of performance.</li>
</ul>
<h2 id="running-alpine-linux-in-qemu">Running Alpine Linux in QEMU</h2>
<p>Let&rsquo;s start by downloading the Alpine Linux installation media.</p>
<div class="highlight"><pre class="chroma"><code class="language-sh" data-lang="sh">$ wget http://dl-cdn.alpinelinux.org/alpine/v3.11/releases/x86_64/alpine-standard-3.11.3-x86_64.iso
</code></pre></div><p>Create a virtual hard drive for the Linux machine. We declare the size to be 10G, but note that only the written sectors will reserve space. The actual size of this file is much smaller.</p>
<div class="highlight"><pre class="chroma"><code class="language-sh" data-lang="sh">$ qemu-img create -f qcow2 alpine.qcow2 10G
</code></pre></div><p>That&rsquo;s all we need for preparation. Now let&rsquo;s move on to boot the installation media.</p>
<div class="highlight"><pre class="chroma"><code class="language-sh" data-lang="sh">$ qemu-system-x86_64 <span class="se">\
</span><span class="se"></span>    -enable-kvm <span class="se">\
</span><span class="se"></span>    -m <span class="m">2048</span> <span class="se">\
</span><span class="se"></span>    -smp <span class="nv">cores</span><span class="o">=</span>2,threads<span class="o">=</span><span class="m">4</span> <span class="se">\
</span><span class="se"></span>    -nic user <span class="se">\
</span><span class="se"></span>    -drive <span class="nv">file</span><span class="o">=</span>alpine.qcow2,media<span class="o">=</span>disk <span class="se">\
</span><span class="se"></span>    -cdrom alpine-standard-3.11.3-x86_64.iso
</code></pre></div><ul>
<li><code>-enable-kvm</code>: Make use of KVM when running a target architecture that is the same as the host architecture. The guest machine can then take advantage of the KVM acceleration.</li>
<li><code>-m 2048</code>: Allocate 2GB memory to guest machine.</li>
<li><code>-smp cores=2,threads=4</code>: Specify the number of CPU cores and threads to use.</li>
<li><code>-nic user</code>: Add a virtual network interface controller to guest machine. More in this <a href="https://www.qemu.org/2018/05/31/nic-parameter/">article</a>.</li>
<li><code>-drive file=alpine.qcow2,media=disk</code>: Attach the newly created virtual hard drive to guest machine. The virtual hard drive will be mounted at <code>/dev/vda</code>.</li>
<li><code>-cdrom alpine-standard-3.11.3-x86_64.iso</code>: Attach a virtual CDROM drive and load Alpine Linux installation media into it.</li>
</ul>
<p>After installing Alpine Linux to the hard drive, we can boot without the <code>-cdrom</code> option.</p>
<div class="highlight"><pre class="chroma"><code class="language-sh" data-lang="sh">$ qemu-system-x86_64 <span class="se">\
</span><span class="se"></span>    -enable-kvm <span class="se">\
</span><span class="se"></span>    -m <span class="m">2048</span> <span class="se">\
</span><span class="se"></span>    -nic user <span class="se">\
</span><span class="se"></span>    -drive <span class="nv">file</span><span class="o">=</span>alpine.qcow2,media<span class="o">=</span>disk
</code></pre></div><p>I&rsquo;m impressed by the flexibility and versatility of QEMU (of course this may come as a downside to some people because there are tons of options to choose).</p>]]></content>
		</item>
		
		<item>
			<title>Publishing Subdirectory to Github Pages</title>
			<link>https://yuankun.me/posts/publishing-subdirectory-to-github-pages/</link>
			<pubDate>Thu, 12 Mar 2020 22:14:58 +0800</pubDate>
			
			<guid>https://yuankun.me/posts/publishing-subdirectory-to-github-pages/</guid>
			<description>&lt;p&gt;I&amp;rsquo;m using Hugo + Github Pages as my personal blog platform. A Hugo site yields the following directory structure, where the &lt;code&gt;public/&lt;/code&gt; subdirectory stores the generated static pages:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;├── archetypes/
├── config.toml
├── content/
├── data/
├── layouts/
├── public/
├── resources/
├── static/
└── themes/
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;How do I publish the &lt;code&gt;public/&lt;/code&gt; subdirectory, instead of the root directory, to Github Pages?&lt;/p&gt;</description>
			<content type="html"><![CDATA[<p>I&rsquo;m using Hugo + Github Pages as my personal blog platform. A Hugo site yields the following directory structure, where the <code>public/</code> subdirectory stores the generated static pages:</p>
<pre><code>├── archetypes/
├── config.toml
├── content/
├── data/
├── layouts/
├── public/
├── resources/
├── static/
└── themes/
</code></pre><p>How do I publish the <code>public/</code> subdirectory, instead of the root directory, to Github Pages?</p>
<p>We need two branches to achieve this. Assuming your default publishing source is the <code>gh-pages</code> branch (check out <a href="https://help.github.com/en/github/working-with-github-pages/about-github-pages#publishing-sources-for-github-pages-sites">this link</a> for more information about default publishing source), we commit the entire Hugo site to the <code>master</code> branch, and commit only the <code>public/</code> subdirectory to the <code>gh-pages</code> branch. Here are the steps:</p>
<ol>
<li>Commit <code>public/</code> to <code>master</code> branch. You need first make sure it isn&rsquo;t ignored by Git.</li>
<li>Push only <code>public/</code> to <code>gh-pages</code> branch: <code>git subtree push --prefix public origin gh-pages</code>.</li>
</ol>
<p>This should do the trick.</p>]]></content>
		</item>
		
	</channel>
</rss>
