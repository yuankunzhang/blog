[{"content":"A coder.\n","permalink":"https://yuankun.me/page/about/","summary":"A coder.","title":"About"},{"content":"A few days ago, I had a home server build. Today I want to have a clean OS reinstallation on my development machine. The primary reason is that the root partition is almost full (due to short-sightness when I first set up this machine). Apart from that, there are several other issues that I want to tackle:\nI got a lot of __common_interrupt: 1.55 No irq handler for vector errors during the system bootup. There\u0026rsquo;s a thread in the Arch Forum discussing this issue. I tried booting up the system with different kernel parameters, but none helped. Seems like the only solution is to upgrade the BIOS firmware. I cannot use my bluetooth keyboard to input decryption key to decrypt the root partition, because the bluetooth service is locked in the root partition and it\u0026rsquo;s a chicken-egg problem. This is quite disturbing, everytime I restarted the machine, I had to wire in my USB keyboard just to input the decryption key. Upgrade BIOS Firmware A word of warning: flashing motherboard BIOS is a dangerous operation. Make sure you read the motherboard manual thoroughly.\nI\u0026rsquo;m using a Gigabyte X570 Aorus Master motherboard for this machine. It has a handy Q-Flash utility embedded in the ROM. The firmware upgrading process is very easy, just follow the manual.\nReinstall the Operating System The installation is not much different from the OS installation on my home server, except that I don\u0026rsquo;t need a RAID and I don\u0026rsquo;t need to remote unlock the disk.\nBut I do want to have my bluetooth keyboard available before the root partition gets decrypted and mounted, so that I can use it to type in the decryption key.\nEnable Bluetooth Keyboard before Disk Decryption We need to bring the bluetooth service into the initramfs. Thankfully, someone has created a mkinitcpio hook for this purpose (kudos to the Arch Linux Community).\nSome notes on this hook:\nIt does not work together with the systemd hook. The project README says that it has only been tested on installation that uses rEFInd as boot loader. I\u0026rsquo;m using systemd-boot (previously called gummiboot), and this hook works just fine. The bluetooth adapter needs to be auto powered on after boot. So I need to add the line AutoEnable=true to the Policy section in the configuration file /etc/bluetooth/main.conf:\n# In /etc/bluetooth/main.conf [Policy] AutoEnable=true And I need to boot the system and manually pair my bluetooth keyboard for a first time. This is a one-off task. Upon next reboot, my keyboard becomes functional just before I\u0026rsquo;m asked to input the disk decryption key.\n","permalink":"https://yuankun.me/posts/dev-machine-setup/","summary":"\u003cp\u003eA few days ago, I had a \u003ca href=\"/posts/home-server-setup\"\u003ehome server build\u003c/a\u003e. Today I want to have a clean OS reinstallation on my development machine. The primary reason is that the root partition is almost full (due to short-sightness when I first set up this machine). Apart from that, there are several other issues that I want to tackle:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eI got a lot of \u003ccode\u003e__common_interrupt: 1.55 No irq handler for vector\u003c/code\u003e errors during the system bootup. There\u0026rsquo;s a thread in the Arch Forum discussing this issue. I tried booting up the system with different kernel parameters, but none helped. Seems like the only solution is to upgrade the BIOS firmware.\u003c/li\u003e\n\u003cli\u003eI cannot use my bluetooth keyboard to input decryption key to decrypt the root partition, because the bluetooth service is locked in the root partition and it\u0026rsquo;s a chicken-egg problem. This is quite disturbing, everytime I restarted the machine, I had to wire in my USB keyboard just to input the decryption key.\u003c/li\u003e\n\u003c/ul\u003e","title":"My Dev Machine Setup"},{"content":"Recently I got some retired computer hardware. Better than putting it in the corner and let it absorb dust, I\u0026rsquo;ve been planning to turn it into a home server. My main goal is to use it primarily as a Samba Server, but I may go futher to run other self-hosted services like NextCloud.\nIn this article I\u0026rsquo;ll talk about my setup of this home server.\nOperating System I\u0026rsquo;ve been using Arch Linux as my desktop OS for a long time, and I\u0026rsquo;m loving it (Why you should also try it). Though it\u0026rsquo;s rare to hear people using Arch Linux as server OS, I\u0026rsquo;d like to give it a try. I will write a review after maybe six months or one year of running it. There are some features that I love the most about Arch Linux:\nMinimal system out of the box. Arch Linux by default has almost nothing installed. You choose what to install. Rolling release. This is what other distros like Debian or CentOS are missing. Because of the rolling release model, my server will be always on the latest everything. Well, it may be a concern that this rolling release model is a bit too aggressive and may introduce unstable features. As I\u0026rsquo;m using it only for my home server, the risk is acceptable. Excellent Wiki. There are detailed guides on almost everything you want to do with your system. Kudos to the community! Hardware Here are my hardware specifications.\nMotherboard: Gigabyte B360M AORUS Gaming 3 CPU: Intel Core i5-8400 (6 cores, 12 threads) RAM: Vengeance LPX 8GB DDR4 DRAM 2400MHz x 4 (32GB in total) SSD: Samsung 980 PRO 1TB PCIe NVMe Gen4 SSD M.2 HDD: Seagate IronWolf 4TB x 2 GPU: Don\u0026rsquo;t have one, don\u0026rsquo;t need one. Goals There are some goals that I want to achieve with this server build.\nNo proprietary software. Except the boot firmware for now. I\u0026rsquo;ll give Libreboot a try later some time. Full disk encryption. Except the boot partition. Disk decryption via a remove SSH session. I don\u0026rsquo;t want to plug in a keyboard and a screen every time I need to restart the server. RAID 1 on the two HDDs. It will be used as the Samba storage partition. I\u0026rsquo;m not sure whether or not it\u0026rsquo;s worth to setup the SSD as a cache for the RAID. For now I\u0026rsquo;m excluding it from my goals. It will be an interesting investigation for future time.\nDisk Partitioning and Encryption Disk partitioning and encryption need to be done before the installation of the operating system.\nThe 1TB SSD are split into two partitions: 1GB of the EFI system partition (mounted at /boot); and the root partition taking up the rest of the SSD.\nDevice Start End Sectors Size Type /dev/nvme0n1p1 2048 2099199 2097152 1G EFI System /dev/nvme0n1p2 2099200 1953525134 1951425935 930.5G Linux filesystem The root partition is encrypted by dm-crypt.\n# Encrypt the root partition. $ cryptsetup luksFormat /dev/nvme0n1p2 # Open the encrypted root partition and mount it to /dev/mapper/root. $ cryptsetup open /dev/nvme0n1p2 root The two HDDs are implemented as software RAID (level 1). I\u0026rsquo;m using mdadm to manage it.\n$ mdadm --create --verbose --level=1 --metadata=1.2 --raid-devices=2 /dev/md0 /dev/sda1 /dev/sdb1 Here, /dev/md0 is the logical RAID block device. It is, in turn, encrypted by dm-crypt. Below is the final layout of the HDDs.\n# Encrypt the RAID block device. $ cryptsetup luksFormat /dev/md0 # Open the RAID block device and mount it to /dev/mapper/nas. $ cryptsetup open /dev/md0 nas Below is the final layout of my disks:\n$ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTS sda 8:0 0 3.6T 0 disk └─sda1 8:1 0 3.6T 0 part └─md0 9:0 0 3.6T 0 raid1 └─nas 254:1 0 3.6T 0 crypt /nas sdb 8:16 0 3.6T 0 disk └─sdb1 8:17 0 3.6T 0 part └─md0 9:0 0 3.6T 0 raid1 └─nas 254:1 0 3.6T 0 crypt /nas nvme0n1 259:0 0 931.5G 0 disk ├─nvme0n1p1 259:1 0 1G 0 part /boot └─nvme0n1p2 259:2 0 930.5G 0 part └─root 254:0 0 930.5G 0 crypt / Mdadm needs the mdadm_udev hook, so we should add it to the HOOKS section in /etc/mkinitcpio.conf:\nArch Linux Installation Follow the great Arch wiki and we are all good.\nI\u0026rsquo;m using systemd-boot as the UEFI boot manager. The CFG Lock BIOS switch must be disabled. The installation is actually very simple:\nMount the EFI system partition at /boot. Run bootctl install to install systemd-boot. This will copy systemd-boot to the EFI partition, and then set systemd-boot as the default EFI boot entry loaded by the EFI Boot manager. Create the bootloader entry. Two files are needed: /boot/loader/loader.conf and /boot/loader/entries/arch.conf. $ cat \u0026lt;\u0026lt;EOF \u0026gt; /boot/loader/loader.conf default arch.conf timeout 3 console-mode max editor no EOF $ cat \u0026lt;\u0026lt;EOF \u0026gt; /boot/loader/entries/arch.conf title Arch Linux linux /vmlinuz-linux initrd /initramfs-linux.img options root=/dev/mapper/root EOF Root Partition Decryption and Mounting One of the goals is for me to be able to remote unlock the LUKS-encrypted root partition. I\u0026rsquo;m using this mkinitcpio hook named systemd-tool. It provides early remote SSH access before the root partition gets mounted.\nFirst, install systemd-tool and its dependencies.\n$ pacman -S mkinitcpio-systemd-tool busybox cryptsetup openssh tinyssh tinyssh-convert mc It requires the systemd-tool hook. Add this hook in /etc/mkinitcpio.conf. The final HOOKS section in /etc/mkinitcpio.conf looks like below.\nHOOKS=(base udev autodetect modconf block mdadm_udev filesystems keyboard fsck systemd systemd-tool) After that, I need to configure /etc/crypttab and /etc/mkinitcpio-systemd-tool/config/crypttab.\n$ echo \u0026#34;crypt UUID=$(blkid -s UUID -o value /dev/sdX2) none luks\u0026#34; \u0026gt; /etc/crypttab $ cat /etc/crypttab \u0026gt; /etc/mkinitcpio-systemd-tool/config/crypttab As well as /etc/fstab and /etc/mkinitcpio-systemd-tool/config/fstab.\n$ echo \u0026#34;UUID=$(blkid -s UUID -o value /dev/mapper/root) / ext4 rw,relatime 0 1\u0026#34; \u0026gt; /etc/fstab $ echo \u0026#34;/dev/mapper/root /sysroot auto x-systemd.device-timeout=9999h 0 1\u0026#34; \u0026gt; /etc/mkinitcpio-systemd-tool/config/fstab Then, enable the required services and build initramfs.\n$ systemctl enable initrd-cryptsetup.path $ systemctl enable initrd-tinysshd $ systemctl enable initrd-debug-progs $ systemctl enable initrd-sysroot-mount $ mkinitcpio -P Remote Unlocking After rebooting, the server now has a fancy remote shell running before the root partition gets mounted. This allows me to connect to the server remotely. A few things to note about this remote shell:\nThe shell is a tinyssh service. Tinyssh only recognizes Ed25519 SSH key. So I need to generate an Ed25519 key pair on my local machine and paste the public key to /root/.ssh/authorized_keys. In this early stage, we can only connect as root user, because other users are not available yet. The root partition will automatically get decrypted and mounted after the decryption key is typed into the prompt.\nI haven\u0026rsquo;t talked about the decryption and mounting of the RAID device. It is actually less a problem. We can simply create a key file under /etc/cryptsetup-keys.d/ (let\u0026rsquo;s call it nas.key) and use this key file to decrypt the RAID device. Modify /etc/crypttab and /etc/fstab as described below and we are all set.\n$ echo \u0026#34;nas UUID=$(blkid -s UUID -o value /dev/md0) /etc/cryptsetup-keys.d/nas.key\u0026#34; \u0026gt;\u0026gt; /etc/crypttab $ echo \u0026#34;UUID=$(blkid -s UUID -o value /dev/mapper/nas) /nas ext4 rw,relatime 0 1\u0026#34; \u0026gt;\u0026gt; /etc/fstab ","permalink":"https://yuankun.me/posts/home-server-setup/","summary":"\u003cp\u003eRecently I got some retired computer hardware. Better than putting it in the corner and let it absorb dust, I\u0026rsquo;ve been planning to turn it into a home server. My main goal is to use it primarily as a Samba Server, but I may go futher to run other self-hosted services like NextCloud.\u003c/p\u003e\n\u003cp\u003eIn this article I\u0026rsquo;ll talk about my setup of this home server.\u003c/p\u003e","title":"My Home Server Setup"},{"content":"There are two types of service accounts in Google Cloud: user-managed service accounts, which are used by user applications to talk to Google Cloud; and Google-managed services accounts, which are used by Google Cloud internally. Among the second category, there is a special subtype of service accounts called Google Cloud Service Agents. Service Agents are used by Google Cloud services to run internal processes so that user requested operations can be fulfilled.\nA service agent has the following pattern:\nservice-PROJECT_NUMBER@SERVICE_NAME.iam.gserviceaccount.com You can spot the service agents from the IAM section of Google Cloud Console.\nWhen managing IAM binding policies via Terraform, these service agents often generate noises. As an example, I\u0026rsquo;ll show you a code snippet coming from one of our Terraform files (I\u0026rsquo;m using xxxxx instead of the real project number).\ndata \u0026#34;google_project\u0026#34; \u0026#34;project\u0026#34; {} resource \u0026#34;google_project_iam_binding\u0026#34; \u0026#34;cloudbuild_service_agent\u0026#34; { project = data.google_project.project.project_id role = \u0026#34;roles/cloudbuild.serviceAgent\u0026#34; members = [ \u0026#34;serviceAccount:service-xxxxx@gcp-sa-cloudbuild.iam.gserviceaccount.com\u0026#34;, ] } resource \u0026#34;google_project_iam_binding\u0026#34; \u0026#34;container_service_agent\u0026#34; { project = data.google_project.project.project_id role = \u0026#34;roles/container.serviceAgent\u0026#34; members = [ \u0026#34;serviceAccount:service-xxxxx@container-engine-robot.iam.gserviceaccount.com\u0026#34;, ] } When you are refering to service agents from other projects, things get even worse. Those project numbers look very much lick what we call \u0026ldquo;the magic number\u0026rdquo;.\nTo tackle this, I wrote a simple Terraform module named google-cloud-service-agents. It consumes a project ID and exposes a list of service agents. With this module, the above code snippet can be rewritten to this:\ndata \u0026#34;google_project\u0026#34; \u0026#34;project\u0026#34; {} module \u0026#34;agents\u0026#34; { source = \u0026#34;/path/to/module\u0026#34; project_number = data.google_project.project.number } resource \u0026#34;google_project_iam_binding\u0026#34; \u0026#34;cloudbuild_service_agent\u0026#34; { project = data.google_project.project.project_id role = \u0026#34;roles/cloudbuild.serviceAgent\u0026#34; members = [ \u0026#34;serviceAccount:${module.agents.cloud_build}\u0026#34;, ] } resource \u0026#34;google_project_iam_binding\u0026#34; \u0026#34;container_service_agent\u0026#34; { project = data.google_project.project.project_id role = \u0026#34;roles/container.serviceAgent\u0026#34; members = [ \u0026#34;serviceAccount:${module.agents.container_engine}\u0026#34;, ] } For more information, please check out the Github repository.\n","permalink":"https://yuankun.me/posts/a-terraform-module-to-list-google-cloud-service-agents/","summary":"\u003cp\u003eThere are \u003ca href=\"https://cloud.google.com/iam/docs/service-accounts#types_of_service_accounts\"\u003etwo types of service accounts\u003c/a\u003e in Google Cloud: user-managed service accounts, which are used by user applications to talk to Google Cloud; and Google-managed services accounts, which are used by Google Cloud internally. Among the second category, there is a special subtype of service accounts called Google Cloud Service Agents. Service Agents are used by Google Cloud services to run internal processes so that user requested operations can be fulfilled.\u003c/p\u003e\n\u003cp\u003eA service agent has the following pattern:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eservice-PROJECT_NUMBER@SERVICE_NAME.iam.gserviceaccount.com\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eYou can spot the service agents from the IAM section of Google Cloud Console.\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/20200402-0012.png\" alt=\"Service Agents\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003eWhen managing IAM binding policies via Terraform, these service agents often generate noises. As an example, I\u0026rsquo;ll show you a code snippet coming from one of our Terraform files (I\u0026rsquo;m using \u003ccode\u003exxxxx\u003c/code\u003e instead of the real project number).\u003c/p\u003e","title":"A Terraform Module to List Google Cloud Service Agents"},{"content":"In last post we see how to run a raw Linux kernel in QEMU. QEMU offers another fancy feature: it can start a GDB Server and external GDB Debugger to connect. With this we can build a comfortable environment to debug system kernels and firmware. Let\u0026rsquo;s see how to leverage this feature to debug the Linux kernel.\nCompiling the Kernel with Debug Info First thing we need to do is to prepare a kernel with debug info. Enter the TUI kernel configuration interface:\n$ cd linux-source/ $ make menuconfig Enter \u0026ldquo;Kernel hacking \u0026gt; Compile-time checks and compiler options\u0026rdquo;, and enable these two options:\nCompile the kernel with debug info Provide GDB scripts for kernel debugging Save the new configuration and compile the kernel by invoking make -j8. After the compilation, we are interested in two newly created files:\nvmlinux. This is the Linux Kernel in an statically linked executable file format with all debugging information. scripts/gdb/vmlinux-gdb.py. This is the GDB script for kernel debugging. Let\u0026rsquo;s add the GDB script to the GDB init file so that the script gets loaded everytime we start GDB Debugger:\n$ echo \u0026#34;add-auto-load-safe-path `pwd`/scripts/gdb/vmlinux-gdb.py\u0026#34; \u0026gt;\u0026gt; ~/.gdbinit Start a Debugging Session QEMU provides two important options for debugging purpose:\nThe -S option prevents the CPU from starting. This gives time for debugger to connect and allows to start debugging from the very beginning. The -s option starts a GDB Server on port 1234. Later in GDB Debugger we can connect to it with target remote :1234. Now let\u0026rsquo;s boot the kernel with these options:\n$ qemu-system-x86_64 \\ -S \\ -s \\ -enable-kvm \\ -kernel bzImage \\ -smp cores=1,threads=2 \\ -m 1024 \\ -append \u0026#34;console=ttyS0 nokaslr selinux=0 debug\u0026#34; \\ -initrd initramfs.img \\ -serial stdio \\ -display none Note that the running of the process is feezed, because we have told QEMU to wait for debugger by using the -S option.\nIn another terminal, start the GDB Debugger and connect it to QEMU:\n$ gdb vmlinux Type \u0026#34;apropos word\u0026#34; to search for commands related to \u0026#34;word\u0026#34;... Reading symbols from vmlinux... (gdb) target remote :1234 Remote debugging using :1234 0x000000000000fff0 in exception_stacks () Now we are able to set break points and trace the running of the kernel as if it is just a normal user application:\n(gdb) b start_kernel Note: breakpoints 1 and 2 also set at pc 0xffffffff829e0aa8. Breakpoint 3 at 0xffffffff829e0aa8: file init/main.c, line 786. (gdb) c Continuing. Thread 1 hit Breakpoint 1, start_kernel () at init/main.c:786 786 { ... References Debugging with QEMU ","permalink":"https://yuankun.me/posts/debug-linux-kernel-with-qemu-and-gdb/","summary":"\u003cp\u003eIn last post we see how to run a raw Linux kernel in QEMU. QEMU offers another fancy feature: it can start a GDB Server and external GDB Debugger to connect. With this we can build a comfortable environment to debug system kernels and firmware. Let\u0026rsquo;s see how to leverage this feature to debug the Linux kernel.\u003c/p\u003e","title":"Debug Linux Kernel With QEMU and GDB"},{"content":"In last post we see how to run a packed Linux distribution in QEMU. This time let\u0026rsquo;s check out how to run a raw Linux kernel in QEMU.\nInitial ramdisk: a very short introduction Many Linux distributions ship a small, generic kernel image. The device drivers are included as loadable kernel modules and stored in file system, it is just not practical to bake all the device drivers into the kernel image. On my machine, the size of vmlinuz-linux is only 6.3 megabytes:\n$ ls -lh /boot/vmlinuz-linux -rw-r--r-- 1 root root 6.3M Mar 15 21:30 /boot/vmlinuz-linux This raises the problem of detecting and loading the modules necessary to mount the root file system at boot time. It\u0026rsquo;s a chicken-and-egg problem. To further complicate the situation, the root file system may require special preparations to mount (for instance, it is on a encrypted partition).\nNow comes the initial ramdisk as a temporary, ram-based root file system. It contains user-space utilities which detect hardwares, descover devices, load necessary modules, and mount the real root file system. Once it is loaded into memory, a simple but sufficient environment is set up for the Linux kernel to complete the boot process. This environment is often called \u0026ldquo;early user space\u0026rdquo;.\nOn my machine, the initial ramdisk image sits in the boot partition with the name initramfs-linux.img. It is larger than the Linux kernel image:\n$ ls -lh /boot/initramfs-linux.img -rw-r--r-- 1 root root 9.4M Mar 15 21:30 /boot/initramfs-linux.img We can inspect contents of the image by using command lsinitcpio /boot/initramfs-linux.img. Indeed, it is a simplified root system with a bunch of helper tools:\n$ lsinitcpio /boot/initramfs-linux.img bin buildconfig config dev/ etc/ etc/fstab etc/initrd-release etc/ld.so.cache etc/ld.so.conf etc/modprobe.d/ etc/mtab hooks/ hooks/udev init init_functions lib lib64 new_root/ proc/ run/ sbin sys/ tmp/ usr/ usr/bin/ ... var/ var/run VERSION Making an initial ramdisk The initial ramdisk creation command in Arch Linux is mkinitcpio, and may defer in other distributions.\nQuoting from man page of mkinitcpio:\n(mkinitcpio) creates an initial ramdisk environment for booting the Linux kernel. The initial ramdisk is in essence a very small environment (early userspace) which loads various kernel modules and sets up necessary things before handing over control to init. This makes it possible to have, for example, encrypted root filesystems and root filesystems on a software RAID array. mkinitcpio allows for easy extension with custom hooks, has autodetection at runtime, and many other features.\nLet\u0026rsquo;s go ahead and create the initial ramdisk.\n$ mkinitcpio -g initramfs.img ==\u0026gt; Starting build: 5.5.9-arch1-2 -\u0026gt; Running build hook: [base] -\u0026gt; Running build hook: [udev] -\u0026gt; Running build hook: [autodetect] -\u0026gt; Running build hook: [modconf] -\u0026gt; Running build hook: [block] -\u0026gt; Running build hook: [filesystems] -\u0026gt; Running build hook: [keyboard] -\u0026gt; Running build hook: [fsck] ==\u0026gt; Generating module dependencies ==\u0026gt; Creating gzip-compressed initcpio image: /home/yuankun/qemu-test/initramfs.img ==\u0026gt; Image generation successful Configuring and building the Linux kernel Clone the Linux source code, and configure the Linux kernel with make ARCH=x86_64 menuconfig.\nSave the configurations and exit the configuration interface, now let\u0026rsquo;s compile the kernel image. The compiled kernel image will be located at arch/x86/boot/bzImage.\n$ make -j8 ... Setup is 13820 bytes (padded to 13824 bytes). System is 8801 kB CRC 52c54fbc Kernel: arch/x86/boot/bzImage is ready (#2) The -j option specifies the number of jobs (commands) to run simultaneously. It\u0026rsquo;s a reasonable choice to match it with your available logical CPU cores.\nBooting the Linux kernel in QEMU Now that we have both the Kernel image and the initial ramdisk, it\u0026rsquo;s time to boot the Linux kernel in QEMU.\n$ qemu-system-x86_64 \\ -enable-kvm \\ -kernel bzImage \\ -smp cores=1,threads=2 \\ -m 1024 \\ -append \u0026#34;console=ttyS0\u0026#34; \\ -initrd initramfs.img \\ -serial stdio \\ -display none [ 0.000000] Linux version 5.6.0-rc6+ (yuankun@mars) (gcc version 9.3.0 (Arch Linux 9.3.0-1)) #2 SMP Tue Mar 17 17:42:13 +08 2020 [ 0.000000] Command line: console=ttyS0 [ 0.000000] x86/fpu: x87 FPU will use FXSAVE [ 0.000000] BIOS-provided physical RAM map: [ 0.000000] BIOS-e820: [mem 0x0000000000000000-0x000000000009fbff] usable [ 0.000000] BIOS-e820: [mem 0x000000000009fc00-0x000000000009ffff] reserved [ 0.000000] BIOS-e820: [mem 0x00000000000f0000-0x00000000000fffff] reserved [ 0.000000] BIOS-e820: [mem 0x0000000000100000-0x000000007ffdffff] usable [ 0.000000] BIOS-e820: [mem 0x000000007ffe0000-0x000000007fffffff] reserved [ 0.000000] BIOS-e820: [mem 0x00000000feffc000-0x00000000feffffff] reserved [ 0.000000] BIOS-e820: [mem 0x00000000fffc0000-0x00000000ffffffff] reserved [ 0.000000] NX (Execute Disable) protection: active [ 0.000000] SMBIOS 2.8 present. ... [rootfs ]# Soon you will find the [rootfs] # prompt appearing, and cheers we are now in the environment provided by the initial ramdisk.\n","permalink":"https://yuankun.me/posts/running-raw-linux-kernel-in-qemu/","summary":"\u003cp\u003eIn last post we see how to run a packed Linux distribution in QEMU. This time let\u0026rsquo;s check out how to run a raw Linux kernel in QEMU.\u003c/p\u003e","title":"Running Raw Linux Kernel in QEMU"},{"content":"How to run a Linux operating system in QEMU.\nQEMU: a very short introduction According to its official site, QEMU is a generic and open source machine emulator and virtualizer.\nQEMU works in one of the two operating modes:\nFull system emulation. In this mode, QEMU emulates a full system, including processors and peripherals. It can be used to launch virtual guest operating systems. User mode emulation. In this mode, QEMU can launch processes that were compiled for a different instruction set. Futhermore, you can provision the -enable-kvm option to leverage Linux KVM. With this option, QEMU deals with the setting up and migration of KVM images and emulates hardwares, and the execution of the guest is done by KVM as requested by QEMU.\nQEMU supports a long list of disk image formats, including the most popular two:\nRaw images (.img). This can be the fastest file-based format. If your file system supports holes, then only the written sectors will reserve space. Use qemu-img info or ls -ls to obtain the real size used by the image. Although raw images give optimal performance, only very basic features are available. QEMU copy on write (.qcow2). This is the most versatile format with advanced feature set. But the feature set comes at the cost of performance. Running Alpine Linux in QEMU Let\u0026rsquo;s start by downloading the Alpine Linux installation media.\n$ wget http://dl-cdn.alpinelinux.org/alpine/v3.11/releases/x86_64/alpine-standard-3.11.3-x86_64.iso Create a virtual hard drive for the Linux machine. We declare the size to be 10G, but note that only the written sectors will reserve space. The actual size of this file is much smaller.\n$ qemu-img create -f qcow2 alpine.qcow2 10G That\u0026rsquo;s all we need for preparation. Now let\u0026rsquo;s move on to boot the installation media.\n$ qemu-system-x86_64 \\ -enable-kvm \\ -m 2048 \\ -smp cores=2,threads=4 \\ -nic user \\ -drive file=alpine.qcow2,media=disk \\ -cdrom alpine-standard-3.11.3-x86_64.iso -enable-kvm: Make use of KVM when running a target architecture that is the same as the host architecture. The guest machine can then take advantage of the KVM acceleration. -m 2048: Allocate 2GB memory to guest machine. -smp cores=2,threads=4: Specify the number of CPU cores and threads to use. -nic user: Add a virtual network interface controller to guest machine. More in this article. -drive file=alpine.qcow2,media=disk: Attach the newly created virtual hard drive to guest machine. The virtual hard drive will be mounted at /dev/vda. -cdrom alpine-standard-3.11.3-x86_64.iso: Attach a virtual CDROM drive and load Alpine Linux installation media into it. After installing Alpine Linux to the hard drive, we can boot without the -cdrom option.\n$ qemu-system-x86_64 \\ -enable-kvm \\ -m 2048 \\ -nic user \\ -drive file=alpine.qcow2,media=disk I\u0026rsquo;m impressed by the flexibility and versatility of QEMU (of course this may come as a downside to some people because there are tons of options to choose).\n","permalink":"https://yuankun.me/posts/running-alpine-linux-in-qemu/","summary":"\u003cp\u003eHow to run a Linux operating system in QEMU.\u003c/p\u003e","title":"Running Alpine Linux in QEMU"},{"content":"I\u0026rsquo;m using Hugo + Github Pages as my personal blog platform. A Hugo site yields the following directory structure, where the public/ subdirectory stores the generated static pages:\n├── archetypes/ ├── config.toml ├── content/ ├── data/ ├── layouts/ ├── public/ ├── resources/ ├── static/ └── themes/ How do I publish the public/ subdirectory, instead of the root directory, to Github Pages?\nWe need two branches to achieve this. Assuming your default publishing source is the gh-pages branch (check out this link for more information about default publishing source), we commit the entire Hugo site to the master branch, and commit only the public/ subdirectory to the gh-pages branch. Here are the steps:\nCommit public/ to master branch. You need first make sure it isn\u0026rsquo;t ignored by Git. Push only public/ to gh-pages branch: git subtree push --prefix public origin gh-pages. This should do the trick.\n","permalink":"https://yuankun.me/posts/publishing-subdirectory-to-github-pages/","summary":"\u003cp\u003eI\u0026rsquo;m using Hugo + Github Pages as my personal blog platform. A Hugo site yields the following directory structure, where the \u003ccode\u003epublic/\u003c/code\u003e subdirectory stores the generated static pages:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e├── archetypes/\n├── config.toml\n├── content/\n├── data/\n├── layouts/\n├── public/\n├── resources/\n├── static/\n└── themes/\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eHow do I publish the \u003ccode\u003epublic/\u003c/code\u003e subdirectory, instead of the root directory, to Github Pages?\u003c/p\u003e","title":"Publishing Subdirectory to Github Pages"},{"content":"YUANKUN ZHANG Linux User\u0026#39;s Manual YUANKUN ZHANG(8) NAME Yuankun Zhang - a programmer SYNOPSIS yuankun [--devops] [--opensource] [--linux-user] [--father] SUMMARY Hi there! I\u0026#39;m a passionate programmer, and I code for fun. I am familiar with DevOps technologies. I use Arch Linux. In my spare time, I like running and playing badminton. OPTIONS --devops I have experience in the DevOps practices and am familar with the tools and approaches used in this space. I prefer statically typed languages (C, Golang, Java) over dynamically typed languages (Python, JavaScript), even though in most of my time I write Python. I\u0026#39;m learning Rust now, and it seems really interesting. --opensource I like to contribute to open source projects whenever I have time. Check out my Github space in the SEE ALSO section for the projects I have been involved in. --linux-user I use Arch Linux as my primary operating system, and run only open source applications. I also have a Ubuntu virtual machine to run work-related, proprierty softwares (such as Zoom and Webex). --father In 2021, I have been propagated from a leaf node to an internal node in my family tree. Being a father is no easy work, but it is the most proud role I\u0026#39;ve ever had. I\u0026#39;m ready to learn and grow together with my new leaf node. SEE ALSO * Github: https://github.com/yuankunzhang * StackOverflow: https://stackoverflow.com/users/756651 * LinkedIn: https://www.linkedin.com/in/yuankun-zhang-b1415b116/ ","permalink":"https://yuankun.me/about/","summary":"YUANKUN ZHANG Linux User\u0026#39;s Manual YUANKUN ZHANG(8) NAME Yuankun Zhang - a programmer SYNOPSIS yuankun [--devops] [--opensource] [--linux-user] [--father] SUMMARY Hi there! I\u0026#39;m a passionate programmer, and I code for fun. I am familiar with DevOps technologies. I use Arch Linux. In my spare time, I like running and playing badminton. OPTIONS --devops I have experience in the DevOps practices and am familar with the tools and approaches used in this space.","title":"Who Am I"}]